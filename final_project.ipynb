{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from gensim import corpora, models\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allemails = pd.read_csv(\"emails.csv\")\n",
    "emails = allemails[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rbanakar/anaconda/lib/python3.6/site-packages/ipykernel/__main__.py:27: DeprecationWarning: Flags not at the start of the expression (?<=X-FileName: )(?s (truncated)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 10)\n"
     ]
    }
   ],
   "source": [
    "def extract_data():\n",
    "    df = pd.DataFrame(columns=['date',\n",
    "                               'sender_address',\n",
    "                               'recipient',\n",
    "                               'subject',\n",
    "                               'sender_name',\n",
    "                               'recipient_name',\n",
    "                               'cc',\n",
    "                               'bcc',\n",
    "                               'folder',\n",
    "                               'body'])\n",
    "    \n",
    "    for index, details in emails.iterrows():\n",
    "        raw_email_info = details['message']\n",
    "        # raw_email_info = f_in.read().decode('utf8')\n",
    "        date = re.findall(r'Date: (.*)', raw_email_info)[0]\n",
    "        sender_address = re.findall(r'From: (.*)', raw_email_info)[0]\n",
    "        recipient = re.findall(r'To: (.*)', raw_email_info)[0]\n",
    "        subject = re.findall(r'Subject: (.*)', raw_email_info)[0]\n",
    "        sender_name = re.findall(r'X-From: (.*)', raw_email_info)[0]\n",
    "        recipient_name = re.findall(r'X-To: ([ A-Za-z]*)', raw_email_info)[0]\n",
    "        cc = re.findall(r'X-cc: (.*)', raw_email_info)[0]\n",
    "        bcc = re.findall(r'X-bcc: (.*)', raw_email_info)[0]\n",
    "        folder = re.findall(r'\"[a-zA-z-]*/(.*)/.*,\"Message-ID.*>', raw_email_info)\n",
    "        \n",
    "        # strip everything before X-FileName\n",
    "        bodies = re.findall(r'(?<=X-FileName: )(?s)(.*$)', raw_email_info)\n",
    "        # Get everything after the first newline\n",
    "        bodies = bodies[0].split('\\n')\n",
    "        body = \" \".join(bodies[1:])\n",
    "        \n",
    "        df.loc[index] = [date, sender_address, recipient, subject, sender_name, recipient_name, cc, bcc, folder, body]\n",
    "        \n",
    "    print(df.shape)\n",
    "    return df\n",
    "                         \n",
    "df = extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = shuffle(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "texts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brad',\n",
       " 'regard',\n",
       " 'tori',\n",
       " 'kuykendal',\n",
       " 'like',\n",
       " 'promot',\n",
       " 'commerci',\n",
       " 'manag',\n",
       " 'instead',\n",
       " 'convert',\n",
       " 'commerci',\n",
       " 'support',\n",
       " 'manag',\n",
       " 'associ',\n",
       " 'duti',\n",
       " 'sinc',\n",
       " 'begin',\n",
       " 'year',\n",
       " 'commerci',\n",
       " 'manag',\n",
       " 'doubt',\n",
       " 'will',\n",
       " 'compar',\n",
       " 'favor',\n",
       " 'other',\n",
       " 'categori',\n",
       " 'year',\n",
       " 'end',\n",
       " 'martin',\n",
       " 'cuilla',\n",
       " 'central',\n",
       " 'desk',\n",
       " 'similiar',\n",
       " 'situat',\n",
       " 'tori',\n",
       " 'hunter',\n",
       " 'like',\n",
       " 'martin',\n",
       " 'handl',\n",
       " 'tori',\n",
       " 'let',\n",
       " 'know',\n",
       " 'issu',\n",
       " 'phillip']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in data['body']:\n",
    "    #print \"Processing\",i\n",
    "    # clean and tokenize document string\n",
    "    tokens = tokenizer.tokenize(i)\n",
    "    # remove all numbers\n",
    "    tokens = [x for x in tokens if not (x.isdigit() or x[0] == '-' and x[1:].isdigit())]\n",
    "    # remove structural words\n",
    "    tokens = [x for x in tokens if len(x) > 1]\n",
    "    tokens = [x.lower() for x in tokens]\n",
    "    tokens = [x for x in tokens if 'http' not in x]\n",
    "    tokens = [x for x in tokens if x not in \"_\"]\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "dictionaryall = corpora.Dictionary(texts)\n",
    "\n",
    "corpusall = [dictionaryall.doc2bow(text) for text in texts]\n",
    "\n",
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forward',\n",
       " 'phillip',\n",
       " 'allen',\n",
       " 'hou',\n",
       " 'ect',\n",
       " 'pm',\n",
       " 'enron',\n",
       " 'admin',\n",
       " 'fsddatasvc',\n",
       " 'com',\n",
       " 'pallen',\n",
       " 'enron',\n",
       " 'com',\n",
       " 'cc',\n",
       " 'subject',\n",
       " 'time',\n",
       " 'sensit',\n",
       " 'execut',\n",
       " 'impact',\n",
       " 'influenc',\n",
       " 'program',\n",
       " 'survey',\n",
       " 'execut',\n",
       " 'impact',\n",
       " 'influenc',\n",
       " 'program',\n",
       " 'immedi',\n",
       " 'action',\n",
       " 'requir',\n",
       " 'delet',\n",
       " 'part',\n",
       " 'execut',\n",
       " 'impact',\n",
       " 'influenc',\n",
       " 'program',\n",
       " 'particip',\n",
       " 'ask',\n",
       " 'gather',\n",
       " 'input',\n",
       " 'particip',\n",
       " 'manag',\n",
       " 'style',\n",
       " 'practic',\n",
       " 'experienc',\n",
       " 'immedi',\n",
       " 'manag',\n",
       " 'direct',\n",
       " 'report',\n",
       " 'eight',\n",
       " 'peer',\n",
       " 'colleagu',\n",
       " 'request',\n",
       " 'provid',\n",
       " 'feedback',\n",
       " 'particip',\n",
       " 'attend',\n",
       " 'next',\n",
       " 'program',\n",
       " 'input',\n",
       " 'self',\n",
       " 'assess',\n",
       " 'manag',\n",
       " 'assess',\n",
       " 'direct',\n",
       " 'report',\n",
       " 'assess',\n",
       " 'peer',\n",
       " 'colleagu',\n",
       " 'assess',\n",
       " 'will',\n",
       " 'combin',\n",
       " 'input',\n",
       " 'other',\n",
       " 'use',\n",
       " 'program',\n",
       " 'particip',\n",
       " 'develop',\n",
       " 'action',\n",
       " 'plan',\n",
       " 'improv',\n",
       " 'manag',\n",
       " 'style',\n",
       " 'practic',\n",
       " 'import',\n",
       " 'complet',\n",
       " 'assess',\n",
       " 'later',\n",
       " 'close',\n",
       " 'busi',\n",
       " 'thursday',\n",
       " 'septemb',\n",
       " 'sinc',\n",
       " 'feedback',\n",
       " 'import',\n",
       " 'part',\n",
       " 'program',\n",
       " 'particip',\n",
       " 'will',\n",
       " 'ask',\n",
       " 'cancel',\n",
       " 'attend',\n",
       " 'enough',\n",
       " 'feedback',\n",
       " 'receiv',\n",
       " 'therefor',\n",
       " 'feedback',\n",
       " 'critic',\n",
       " 'complet',\n",
       " 'assess',\n",
       " 'pleas',\n",
       " 'click',\n",
       " 'follow',\n",
       " 'link',\n",
       " 'simpli',\n",
       " 'open',\n",
       " 'internet',\n",
       " 'browser',\n",
       " 'go',\n",
       " 'www',\n",
       " 'fsddatasvc',\n",
       " 'com',\n",
       " 'enron',\n",
       " 'uniqu',\n",
       " 'id',\n",
       " 'particip',\n",
       " 'ask',\n",
       " 'rate',\n",
       " 'uniqu',\n",
       " 'id',\n",
       " 'particip',\n",
       " 'evh3ji',\n",
       " 'john',\n",
       " 'arnold',\n",
       " 'er93fx',\n",
       " 'john',\n",
       " 'lavorato',\n",
       " 'epexwx',\n",
       " 'hunter',\n",
       " 'shive',\n",
       " 'experi',\n",
       " 'technic',\n",
       " 'problem',\n",
       " 'pleas',\n",
       " 'call',\n",
       " 'denni',\n",
       " 'ward',\n",
       " 'fsd',\n",
       " 'data',\n",
       " 'servic',\n",
       " 'question',\n",
       " 'process',\n",
       " 'may',\n",
       " 'contact',\n",
       " 'debbi',\n",
       " 'nowak',\n",
       " 'enron',\n",
       " 'christi',\n",
       " 'smith',\n",
       " 'keilti',\n",
       " 'goldsmith',\n",
       " 'compani',\n",
       " 'thank',\n",
       " 'particip']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ldamodelall = gensim.models.ldamodel.LdaModel(corpusall, num_topics=7, id2word = dictionaryall, passes=20,\n",
    "                                              minimum_probability=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, '0.013*\"phillip\" + 0.013*\"can\" + 0.012*\"will\" + 0.012*\"ga\" + 0.012*\"price\"'), (5, '0.151*\"enron\" + 0.042*\"ect\" + 0.036*\"na\" + 0.034*\"corp\" + 0.022*\"hou\"'), (1, '0.021*\"project\" + 0.014*\"will\" + 0.013*\"austin\" + 0.012*\"phillip\" + 0.009*\"properti\"'), (4, '0.026*\"ect\" + 0.016*\"com\" + 0.016*\"phillip\" + 0.013*\"hou\" + 0.012*\"enron\"'), (6, '0.022*\"phillip\" + 0.016*\"hotmail\" + 0.015*\"com\" + 0.014*\"can\" + 0.014*\"luci\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodelall.print_topics(num_topics=5, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
